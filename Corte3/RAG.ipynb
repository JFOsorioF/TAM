{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juan Felipe Osorio Franco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationSummaryBufferMemory\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Set up OpenAI API key\n",
    "OPENAI_API_KEY = getpass('Enter your OpenAI API key: ')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "# Load and process document\n",
    "loader = PyPDFLoader(\"ACUERDO_008_2008_CSU_Estatuto_Estudiantil.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"rag_database\"\n",
    ")\n",
    "\n",
    "# Initialize base LLM\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicios:\n",
    "\n",
    "1. Qué pasa si el elemento de memoria se vuelve muy extenso? En tal caso consultar e implementar:\n",
    "- Summary Memory.\n",
    "- Buffer-Summary Memory.\n",
    "\n",
    "2. Será que todas las preguntas tienen k=5 fragmentos de textos relacionados? Implementar un sistema de RAG Adaptativo; para esto se debe implementar un LLM que filtre los fragmentos de texto que en realidad son relevantes a la hora de responder la pregunta hecha por el usuario.\n",
    "\n",
    "3. Implementar un Chatbot con memoria a corto plazo que responda preguntas acerca del paper que se estén leyendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que la memoria sea muy grande ariesgamos que el modelo olvide informacion o que choquemos contral el limite de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"\n",
    "Se te proporcionará una serie de textos que dan instrucciones acerca de como resolver preguntas \n",
    "a un estudiante de la universidad Nacional de Colombia. Responde de la manera más completa posible.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Historial de la conversación:\n",
    "{chat_history}\n",
    "\n",
    "Pregunta: {human_input}\n",
    "Respuesta formal:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"human_input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Initialize both memory types\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "buffer_summary_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=2000,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create chains with different memory types\n",
    "summary_chain = load_qa_chain(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    memory=summary_memory,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "buffer_summary_chain = load_qa_chain(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    memory=buffer_summary_memory,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "def get_response(question, chain_type=\"summary\"):\n",
    "    # Choose the appropriate chain\n",
    "    chain = summary_chain if chain_type == \"summary\" else buffer_summary_chain\n",
    "    \n",
    "    # Get relevant documents\n",
    "    docs = vectorstore.similarity_search(question, k=5)\n",
    "    \n",
    "    # Get response\n",
    "    response = chain({\"input_documents\": docs, \"human_input\": question})\n",
    "    \n",
    "    return response[\"output_text\"]\n",
    "\n",
    "# Example usage\n",
    "question = \"¿Qué es la admisión?\"\n",
    "print(\"\\nRespuesta (usando Summary Memory):\")\n",
    "print(get_response(question, \"summary\"))\n",
    "print(\"\\nRespuesta (usando Buffer-Summary Memory):\")\n",
    "print(get_response(question, \"buffer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relevance filter prompt\n",
    "filter_template = \"\"\"\n",
    "Evalúa si el siguiente contexto es relevante para responder la pregunta.\n",
    "Responde únicamente con 'Relevante' o 'No Relevante'.\n",
    "\n",
    "Pregunta: {question}\n",
    "Contexto: {context}\n",
    "\n",
    "Relevancia:\"\"\"\n",
    "\n",
    "filter_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=filter_template\n",
    ")\n",
    "\n",
    "# Create filter chain\n",
    "filter_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=filter_prompt\n",
    ")\n",
    "\n",
    "# Create QA prompt\n",
    "qa_template = \"\"\"\n",
    "Basado en el siguiente contexto relevante, responde la pregunta del estudiante:\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Historial:\n",
    "{chat_history}\n",
    "\n",
    "Pregunta: {human_input}\n",
    "Respuesta formal:\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"human_input\"],\n",
    "    template=qa_template\n",
    ")\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"human_input\"\n",
    ")\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = load_qa_chain(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    prompt=qa_prompt\n",
    ")\n",
    "\n",
    "def get_relevant_documents(question, k=10):\n",
    "    \"\"\"Retrieve and filter relevant documents\"\"\"\n",
    "    # Get initial documents\n",
    "    docs = vectorstore.similarity_search(question, k=k)\n",
    "    \n",
    "    relevant_docs = []\n",
    "    for doc in docs:\n",
    "        # Check relevance\n",
    "        result = filter_chain.run(\n",
    "            question=question,\n",
    "            context=doc.page_content\n",
    "        )\n",
    "        \n",
    "        if result.strip().lower() == 'relevante':\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "def get_adaptive_response(question):\n",
    "    \"\"\"Get response using adaptive document retrieval\"\"\"\n",
    "    # Get filtered relevant documents\n",
    "    relevant_docs = get_relevant_documents(question)\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return \"No encontré información relevante para responder tu pregunta.\"\n",
    "    \n",
    "    # Get response using relevant documents\n",
    "    response = qa_chain({\n",
    "        \"input_documents\": relevant_docs,\n",
    "        \"human_input\": question,\n",
    "        \"chat_history\": memory\n",
    "    })\n",
    "    \n",
    "    return response[\"output_text\"]\n",
    "\n",
    "# Example usage\n",
    "question = \"¿Cuáles son los requisitos de admisión?\"\n",
    "print(\"\\nRespuesta adaptativa:\")\n",
    "print(get_adaptive_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom prompt for research paper QA\n",
    "paper_template = \"\"\"\n",
    "Actúa como un asistente experto que está familiarizado con el contenido del artículo de investigación.\n",
    "Utiliza el siguiente contexto y el historial de la conversación para responder la pregunta de manera clara y precisa.\n",
    "Si no estás seguro de algo, indícalo honestamente.\n",
    "\n",
    "Contexto del artículo:\n",
    "{context}\n",
    "\n",
    "Historial de la conversación:\n",
    "{chat_history}\n",
    "\n",
    "Pregunta: {question}\n",
    "Respuesta detallada:\"\"\"\n",
    "\n",
    "# Initialize memory for paper chat\n",
    "paper_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create the chain for paper QA\n",
    "paper_qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0.3),\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    ),\n",
    "    memory=paper_memory,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def get_paper_response(question):\n",
    "    \"\"\"Get response for a research paper question\"\"\"\n",
    "    result = paper_qa({\"question\": question})\n",
    "    return {\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"sources\": [doc.metadata for doc in result[\"source_documents\"]]\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "question = \"¿Cuál es el principal argumento del documento?\"\n",
    "response = get_paper_response(question)\n",
    "print(\"\\nRespuesta:\", response[\"answer\"])\n",
    "print(\"\\nFuentes utilizadas:\")\n",
    "for source in response[\"sources\"]:\n",
    "    print(f\"- Página {source.get('page', 'N/A')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
